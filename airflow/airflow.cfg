[core]
# La carpeta donde tus DAGs serán almacenados
dags_folder = /opt/airflow/dags
# Si los DAGs deben estar pausados al crearse por primera vez. No afecta a los DAGs ya existentes.
dags_are_paused_at_creation = True
# No cargar los ejemplos después de la inicialización de la base de datos.
load_examples = False
# El ejecutor que deberías usar, puede ser SequentialExecutor, LocalExecutor, CeleryExecutor, etc.
executor = LocalExecutor

[database]
# Asegúrate de que esta línea está completa y correcta
sql_alchemy_conn = postgresql+psycopg2://${PG_USER:-airflow}:${PG_PASSWORD:-airflow}@postgres/${PG_DATABASE:-airflow}

[logging]
# Configuración de los logs de Airflow
log_level = INFO
base_log_folder = /opt/airflow/logs
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
log_filename_template = %%{{ ti.dag_id }}%%/%%{{ ti.task_id }}%%/%%{{ ts }}%%/%%{{ try_number }}%%.log

[webserver]
# La URL base para acceder al servidor web de Airflow
base_url = http://localhost:8081
# El host en donde escuchará el servidor web
web_server_host = 0.0.0.0
# El puerto para el servidor web
web_server_port = 8081
# Si quieres iniciar el servidor web en modo de depuración
web_server_debug = False
# La llave secreta usada para ejecutar tu aplicación flask
secret_key = base64encodedFernetKeyHere
authenticate = False
auth_backend = airflow.contrib.auth.backends.password_auth

[scheduler]
# El tiempo en segundos de cómo a menudo el scheduler hace un "heartbeat"
scheduler_heartbeat_sec = 5
# Tras cuánto tiempo de inactividad se considera que un scheduler está muerto
scheduler_health_check_threshold = 30
# Cuántos procesos puede ejecutar el Scheduler simultáneamente
max_threads = 2

[operators]
# El propietario por defecto asignado a los DAGs
default_owner = Airflow

[api]
# Habilitar o deshabilitar la API experimental
enable_experimental_api = True

[admin]
# Locales que deseas habilitar en la interfaz de Airflow
hide_paused_dags_by_default = False
