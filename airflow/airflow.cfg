[core]
# Home directory for airflow (Set via environment variable, not here)
# airflow_home = /usr/local/airflow

dags_are_paused_at_creation = True
load_examples = False

[database]
# The SQLAlchemy connection string to the metadata database.
# Updated section from [core] to [database] to adhere to new Airflow configurations.
sql_alchemy_conn = postgresql+psycopg2://${PG_USER:-airflow}:${PG_PASSWORD:-airflow}@postgres/${PG_DATABASE:-airflow}

[logging]
# Logging level
log_level = INFO

# Directory where logs will be stored
base_log_folder = /usr/local/airflow/logs

# Log format corrected with double % to escape percent signs
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Updated log file template to comply with the new default value
log_filename_template = %%{{ ti.dag_id }}%%/%%{{ ti.task_id }}%%/%%{{ ts }}%%/%%{{ try_number }}%%.log

[webserver]
# The base url of your website as airflow cannot guess
base_url = http://localhost:${AIRFLOW_PORT:-8080}

# The ip to bind to (use 0.0.0.0 to bind to all IPs)
web_server_host = 0.0.0.0

# The port to run the webserver on
web_server_port = ${AIRFLOW_PORT:-8080}

# Secret key to save connection
# Secret key for establishing encrypted connections
secret_key = base64encodedFernetKeyHere

[scheduler]
# Scheduler job heartbeat interval, in seconds
scheduler_heartbeat_sec = 5

# How often, in seconds, to check if the scheduler heartbeat is still alive
scheduler_health_check_threshold = 30

[operators]
# Default owner assigned to any new DAGs
default_owner = Airflow
